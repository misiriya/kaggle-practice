{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport os\nfrom copy import deepcopy\nfrom functools import partial\nfrom itertools import combinations\nimport random\nimport gc\n\n# Import sklearn classes for model selection, cross validation, and performance evaluation\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import ensemble\nimport seaborn as sns\nfrom category_encoders import OneHotEncoder, OrdinalEncoder, CountEncoder, CatBoostEncoder\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Import libraries for Hypertuning\nimport optuna\n\n# Import libraries for gradient boosting\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom catboost import CatBoost, CatBoostRegressor, CatBoostClassifier\nfrom catboost import Pool\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)","metadata":{"_uuid":"02ee824a-f566-4b40-a116-44e045ce5fef","_cell_guid":"1749ee86-ec3d-4419-8eed-5815b38878d4","collapsed":false,"execution":{"iopub.status.busy":"2023-03-30T14:09:07.362446Z","iopub.execute_input":"2023-03-30T14:09:07.363987Z","iopub.status.idle":"2023-03-30T14:09:13.329314Z","shell.execute_reply.started":"2023-03-30T14:09:07.363929Z","shell.execute_reply":"2023-03-30T14:09:13.328075Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data","metadata":{"_uuid":"c4490772-af78-42b9-b55d-67cbb35ae72b","_cell_guid":"69dfe3a8-b43b-46a2-9182-fea5caaf0bcd","trusted":true}},{"cell_type":"code","source":"def fix_columns(df): \n    \"\"\"Removes (in millions) and (approx).1 from names of columns.\"\"\"\n    df.columns = df.columns.str.replace('(in millions)', '', regex=False)\n    df.columns = df.columns.str.replace(' home(approx).1', '_home', regex=False)\n    return df\n\nfilepath = '/kaggle/input/playground-series-s3e11'\n\ndf_train = pd.read_csv(os.path.join(filepath, 'train.csv'), index_col=[0])\ndf_test = pd.read_csv(os.path.join(filepath, 'test.csv'), index_col=[0])\noriginal = pd.read_csv('/kaggle/input/media-campaign-cost-prediction/train_dataset.csv')\n\ndf_train = fix_columns(df_train)\ndf_test = fix_columns(df_test)\noriginal = fix_columns(original)\n\ndf_train['is_generated'] = 1\ndf_test['is_generated'] = 1\noriginal['is_generated'] = 0\n\noriginal = original.reset_index()\noriginal['id'] = original['index'] + df_test.index[-1] + 1\noriginal = original.drop(columns = ['index']).set_index('id')\n\ntarget_col = 'cost'","metadata":{"_uuid":"3a79dc94-c0e6-4511-b19a-c4e80be12c31","_cell_guid":"83fea068-f612-4bd4-9957-9a7729e945e9","collapsed":false,"execution":{"iopub.status.busy":"2023-03-30T14:09:13.331566Z","iopub.execute_input":"2023-03-30T14:09:13.332430Z","iopub.status.idle":"2023-03-30T14:09:14.889346Z","shell.execute_reply.started":"2023-03-30T14:09:13.332374Z","shell.execute_reply":"2023-03-30T14:09:14.888268Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Simple EDA","metadata":{"_uuid":"777fb4d4-25ec-4e0d-97e4-15941b109d6f","_cell_guid":"93b3513a-95ef-4276-a807-93b041eeadcd","trusted":true}},{"cell_type":"code","source":"n_cols = 2\nn_rows = (len(df_test.columns) - 1) // n_cols + 1\n\nfig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(20, 30))\n\nfor i, var_name in enumerate(df_test.columns.tolist()):\n    row = i // n_cols\n    col = i % n_cols\n    \n    ax = axes[row, col]\n    sns.distplot(df_train[var_name], kde=True, ax=ax, label='Train')\n    sns.distplot(df_test[var_name], kde=True, ax=ax, label='Test')\n    sns.distplot(original[var_name], kde=True, ax=ax, label='Original')\n    ax.set_title(f'{var_name} Distribution (Train vs Test)')\n    ax.legend()\n    \nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"9e64985a-7f4c-4d16-beb3-43997c38abf8","_cell_guid":"fdc9e62c-8512-4247-bd11-d8eb2c1bc6d7","collapsed":false,"execution":{"iopub.status.busy":"2023-03-30T14:09:14.890906Z","iopub.execute_input":"2023-03-30T14:09:14.891252Z","iopub.status.idle":"2023-03-30T14:09:56.319714Z","shell.execute_reply.started":"2023-03-30T14:09:14.891217Z","shell.execute_reply":"2023-03-30T14:09:56.318743Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(18, 10))\nmask = np.triu(np.ones_like(df_train.drop('is_generated', axis=1).corr()))\nsns.heatmap(df_train.drop('is_generated', axis=1).corr(),cmap='YlOrRd',annot=True,mask=mask)","metadata":{"_uuid":"babfd4d6-2a59-4b9d-8e0a-c7c959c25fe1","_cell_guid":"f1bbdfb1-2f93-4c2a-b110-57515e7a466a","collapsed":false,"execution":{"iopub.status.busy":"2023-03-30T14:09:56.322026Z","iopub.execute_input":"2023-03-30T14:09:56.322368Z","iopub.status.idle":"2023-03-30T14:09:58.212907Z","shell.execute_reply.started":"2023-03-30T14:09:56.322324Z","shell.execute_reply":"2023-03-30T14:09:58.211734Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare train and test sets","metadata":{"_uuid":"a384d535-58cf-4ee1-9810-16124065c2e3","_cell_guid":"383534c3-e93b-4331-b253-f857908f31a6","trusted":true}},{"cell_type":"code","source":"def fe(df):\n    # kudos to https://www.kaggle.com/code/sergiosaharovskiy/ps-s3e11-2023-eda-and-submission\n    df.unit_sales = df.unit_sales.clip(0, 5)\n    df['children_ratio'] = df['total_children'] / df['num_children_at_home']\n    df['children_ratio'] = df['children_ratio'].replace([np.inf, -np.inf], 10)\n    return df\n\n# Apply FE\ndf_train = fe(df_train)\noriginal = fe(original)\ndf_test = fe(df_test)\n\n# Concatenate train and original dataframes, and prepare train and test sets\ndf_train = pd.concat([df_train, original])\nX_train = df_train.drop([f'{target_col}'],axis=1).reset_index(drop=True)\ny_train = df_train[f'{target_col}'].reset_index(drop=True)\nX_test = df_test.reset_index(drop=True)\n\n# Transform the values in y_train using the natural logarithm function\ny_train_ori = y_train.copy()\ny_train = np.log1p(y_train_ori)\n\n# Drop cols\ncols_to_drop = [\n    'store_sales', \n    'gross_weight', \n    'unit_sales', \n    'low_fat',\n    'recyclable_package', \n    'salad_bar', \n    'units_per_case'\n]\nX_train.drop(cols_to_drop, axis=1, inplace=True)\nX_test.drop(cols_to_drop, axis=1, inplace=True)\n\nprint(f\"X_train shape :{X_train.shape} , y_train shape :{y_train.shape}\")\nprint(f\"X_test shape :{X_test.shape}\")\n\n# Delete the train and test dataframes to free up memory\ndel df_train, df_test","metadata":{"_uuid":"f654ab43-1ea1-438a-8245-0d4bd05a2470","_cell_guid":"2fe0af96-eba3-478e-bd73-56c789d386da","collapsed":false,"execution":{"iopub.status.busy":"2023-03-30T14:09:58.214673Z","iopub.execute_input":"2023-03-30T14:09:58.215328Z","iopub.status.idle":"2023-03-30T14:09:58.413906Z","shell.execute_reply.started":"2023-03-30T14:09:58.215290Z","shell.execute_reply":"2023-03-30T14:09:58.412635Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_columns = [\n    'total_children', \n    'num_children_at_home', \n    'avg_cars_at_home', \n    'store_sqft', \n    'coffee_bar', \n    'video_store', \n    'prepared_food'\n]\n\nfor col in categorical_columns:\n    X_train[col] = X_train[col].astype(int)\n    X_test[col] = X_test[col].astype(int)","metadata":{"_uuid":"2d501ee3-c9a9-418a-865b-71a366ba1596","_cell_guid":"e47e18e6-d1f2-4465-9b90-86828373fa4b","collapsed":false,"execution":{"iopub.status.busy":"2023-03-30T14:09:58.415433Z","iopub.execute_input":"2023-03-30T14:09:58.415806Z","iopub.status.idle":"2023-03-30T14:09:58.484946Z","shell.execute_reply.started":"2023-03-30T14:09:58.415768Z","shell.execute_reply":"2023-03-30T14:09:58.483925Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Splitter:\n    def __init__(self, test_size=0.2, kfold=True, n_splits=5):\n        self.test_size = test_size\n        self.kfold = kfold\n        self.n_splits = n_splits\n\n    def split_data(self, X, y, random_state_list):\n        if self.kfold:\n            for random_state in random_state_list:\n                kf = KFold(n_splits=self.n_splits, random_state=random_state, shuffle=True)\n                for train_index, val_index in kf.split(X, y):\n                    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n                    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n                    yield X_train, X_val, y_train, y_val\n        else:\n            for random_state in random_state_list:\n                X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.test_size, random_state=random_state)\n                yield X_train, X_val, y_train, y_val\n\nkfold = True\nn_splits = 1 if not kfold else 15\nrandom_state = 42\nrandom_state_list = [42] # used by split_data [42, 41, 35]\nn_estimators = 9999 # 9999\nearly_stopping_rounds = 200\nverbose = False\ndevice = 'gpu'\n\nsplitter = Splitter(kfold=kfold, n_splits=n_splits)","metadata":{"_uuid":"5e2b0869-d825-4a78-8757-f6c59c2c0ba0","_cell_guid":"24c1fe68-5f99-4dfd-a40d-a6b8ceeee6d4","collapsed":false,"execution":{"iopub.status.busy":"2023-03-30T14:09:58.486385Z","iopub.execute_input":"2023-03-30T14:09:58.486776Z","iopub.status.idle":"2023-03-30T14:09:58.497725Z","shell.execute_reply.started":"2023-03-30T14:09:58.486739Z","shell.execute_reply":"2023-03-30T14:09:58.496337Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Model","metadata":{"_uuid":"8cd711f4-b673-4f37-993c-30f06ea428ee","_cell_guid":"5d2cdbf2-8ced-46ab-8465-238e90e5a569","trusted":true}},{"cell_type":"code","source":"class Regressor:\n    def __init__(self, n_estimators=100, device=\"cpu\", random_state=0):\n        self.n_estimators = n_estimators\n        self.device = device\n        self.random_state = random_state\n        self.reg_models = self._define_reg_model()\n        self.len_models = len(self.reg_models)\n        \n    def _define_reg_model(self):\n        \n        xgb_params = {\n            'n_estimators': self.n_estimators,\n            'learning_rate': 0.05,\n            'max_depth': 8,\n            'subsample': 1.0,\n            'colsample_bytree': 1.0,\n            'n_jobs': -1,\n            'objective': 'reg:squarederror',\n            'verbosity': 0,\n            'eval_metric': 'rmse',\n            'random_state': self.random_state,\n        }\n        if self.device == 'gpu':\n            xgb_params['tree_method'] = 'gpu_hist'\n            xgb_params['predictor'] = 'gpu_predictor'\n        \n        lgb_params = {\n            'n_estimators': self.n_estimators,\n            'max_depth': 8,\n            'learning_rate': 0.05293702575527996,\n            'subsample': 0.20851841295589477,\n            'colsample_bytree': 0.5784778854092203,\n            'reg_alpha': 0.2622912287429849,\n            'reg_lambda': 2.8702494234117617e-08,\n            'objective': 'regression',\n            'metric': 'rmse',\n            'boosting_type': 'gbdt',\n            'device': self.device,\n            'random_state': self.random_state\n        }\n        \n        cb_params = {\n            'iterations': self.n_estimators,\n            'depth': 7,\n            'learning_rate': 0.12947105266151432,\n            'l2_leaf_reg': 0.6169164517797081,\n            'random_strength': 0.21235850198764036,\n            'max_bin': 212,\n            'od_wait': 67,\n            'one_hot_max_size': 73,\n            'grow_policy': 'Depthwise',\n            'bootstrap_type': 'Bayesian',\n            'od_type': 'Iter',\n            'loss_function': 'RMSE',\n            'task_type': self.device.upper(),\n            'random_state': self.random_state\n        }\n        \n        reg_models = {\n            'xgb_reg': xgb.XGBRegressor(**xgb_params),\n            'lgb_reg': lgb.LGBMRegressor(**lgb_params),\n            'cat_reg': CatBoostRegressor(**cb_params)\n        }\n\n        return reg_models","metadata":{"_uuid":"aee1ee23-8e86-46aa-895e-6949ba3b60b8","_cell_guid":"ec33550c-5c30-4ec3-87ac-2fe7ff67c5fd","collapsed":false,"execution":{"iopub.status.busy":"2023-03-30T14:09:58.502154Z","iopub.execute_input":"2023-03-30T14:09:58.502719Z","iopub.status.idle":"2023-03-30T14:09:58.515547Z","shell.execute_reply.started":"2023-03-30T14:09:58.502678Z","shell.execute_reply":"2023-03-30T14:09:58.514197Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class OptunaWeights:\n    def __init__(self, random_state):\n        self.study = None\n        self.weights = None\n        self.random_state = random_state\n\n    def _objective(self, trial, y_true, y_preds):\n        # Define the weights for the predictions from each model\n        weights = [trial.suggest_float(f\"weight{n}\", 0, 1) for n in range(len(y_preds))]\n\n        # Calculate the weighted prediction\n        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=weights)\n\n        # Calculate the RMSLE score for the weighted prediction\n        score = np.sqrt(mean_squared_log_error(y_true, weighted_pred))\n        return score\n\n    def fit(self, y_true, y_preds, n_trials=300):\n        optuna.logging.set_verbosity(optuna.logging.ERROR)\n        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n        self.study = optuna.create_study(sampler=sampler, study_name=\"OptunaWeights\", direction='minimize')\n        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n        self.study.optimize(objective_partial, n_trials=n_trials)\n        self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n\n    def predict(self, y_preds):\n        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=self.weights)\n        return weighted_pred\n\n    def fit_predict(self, y_true, y_preds, n_trials=300):\n        self.fit(y_true, y_preds, n_trials=n_trials)\n        return self.predict(y_preds)\n    \n    def weights(self):\n        return self.weights","metadata":{"_uuid":"72727e1f-7b54-4635-909c-9e12cc000932","_cell_guid":"d08ce145-2ccc-40a6-98da-c587f7a4d15b","collapsed":false,"execution":{"iopub.status.busy":"2023-03-30T14:09:58.517182Z","iopub.execute_input":"2023-03-30T14:09:58.517600Z","iopub.status.idle":"2023-03-30T14:09:58.533068Z","shell.execute_reply.started":"2023-03-30T14:09:58.517561Z","shell.execute_reply":"2023-03-30T14:09:58.531856Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Model","metadata":{"_uuid":"478b5b84-d8e5-42b2-a609-a1f204264ff5","_cell_guid":"dda54451-e4d9-4d24-853a-0ddb6466db93","trusted":true}},{"cell_type":"code","source":"# Initialize an array for storing test predictions\ntest_predss = np.zeros(X_test.shape[0])\nensemble_score = []\nweights = []\ntrained_models = dict(zip(Regressor().reg_models.keys(), [[] for _ in range(Regressor().len_models)]))\n\n# Evaluate on validation data and store predictions on test data\nfor i, (X_train_, X_val, y_train_, y_val) in enumerate(splitter.split_data(X_train, y_train, random_state_list=random_state_list)):\n    n = i % n_splits\n    m = i // n_splits\n        \n    # Get a set of Regressor models\n    reg = Regressor(n_estimators, device, random_state)\n    models = reg.reg_models\n    \n    # Initialize lists to store oof and test predictions for each base model\n    oof_preds = []\n    test_preds = []\n    \n    # Loop over each base model and fit it to the training data, evaluate on validation data, and store predictions\n    for name, model in models.items():\n        if name == 'cat_reg':\n            train_pool = Pool(X_train_, y_train_, cat_features=categorical_columns)\n            test_pool = Pool(X_val, y_val ,cat_features=categorical_columns)\n            model.fit(train_pool, eval_set=[test_pool], early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n        elif name == 'lgb_reg':\n            model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)], \n                      categorical_feature=categorical_columns, early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n        else:\n            model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)], early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n        y_val_pred = model.predict(X_val)\n        test_pred = model.predict(X_test)\n        \n        # Convert predicted values back to their original scale by applying the expm1 function\n        y_val_pred = np.expm1(y_val_pred)\n        test_pred = np.expm1(test_pred)\n        \n        score = np.sqrt(mean_squared_log_error(np.expm1(y_val), y_val_pred))\n        print(f'{name} [FOLD-{n} SEED-{random_state_list[m]}] RMSLE score: {score:.5f}')\n        \n        oof_preds.append(y_val_pred)\n        test_preds.append(test_pred)\n        trained_models[f'{name}'].append(deepcopy(model))\n    \n    # Use Optuna to find the best ensemble weights\n    y_val = np.expm1(y_val)\n    optweights = OptunaWeights(random_state=random_state)\n    y_val_pred = optweights.fit_predict(y_val.values, oof_preds)\n    score = np.sqrt(mean_squared_log_error(y_val, y_val_pred))\n    print(f'Ensemble [FOLD-{n} SEED-{random_state_list[m]}] RMSLE score {score:.5f}')\n    ensemble_score.append(score)\n    weights.append(optweights.weights)\n    test_predss += optweights.predict(test_preds) / (n_splits * len(random_state_list))\n    \ngc.collect()","metadata":{"_uuid":"3e5b7b9e-a4e4-4a24-b51e-c1239be610cb","_cell_guid":"d6bbbadb-1dff-4419-b27f-00b3fde931d7","collapsed":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-03-30T14:09:58.537404Z","iopub.execute_input":"2023-03-30T14:09:58.537807Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the mean LogLoss score of the ensemble\nmean_score = np.mean(ensemble_score)\nstd_score = np.std(ensemble_score)\nprint(f'Ensemble RMSLE score {mean_score:.5f} ± {std_score:.5f}')\n\n# Print the mean and standard deviation of the ensemble weights for each model\nprint('--- Model Weights ---')\nmean_weights = np.mean(weights, axis=0)\nstd_weights = np.std(weights, axis=0)\nfor name, mean_weight, std_weight in zip(models.keys(), mean_weights, std_weights):\n    print(f'{name} {mean_weight:.5f} ± {std_weight:.5f}')","metadata":{"_uuid":"84c1d933-6c3f-4d2e-94f5-89f9d12e8f5c","_cell_guid":"f5464cf2-9429-4a27-a89d-2943feb0be4f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_importance(models, feature_cols, title, top=16):\n    importances = []\n    feature_importance = pd.DataFrame()\n    for i, model in enumerate(models):\n        _df = pd.DataFrame()\n        _df[\"importance\"] = model.feature_importances_\n        _df[\"feature\"] = pd.Series(feature_cols)\n        _df[\"fold\"] = i\n        _df = _df.sort_values('importance', ascending=False)\n        _df = _df.head(top)\n        feature_importance = pd.concat([feature_importance, _df], axis=0, ignore_index=True)\n        \n    feature_importance = feature_importance.sort_values('importance', ascending=False)\n    # display(feature_importance.groupby([\"feature\"]).mean().reset_index().drop('fold', axis=1))\n    plt.figure(figsize=(16, 10))\n    sns.barplot(x='importance', y='feature', data=feature_importance, color='skyblue', errorbar='sd')\n    plt.xlabel('Importance', fontsize=14)\n    plt.ylabel('Feature', fontsize=14)\n    plt.title(f'{title} Feature Importance [Top {top}]', fontsize=18)\n    plt.grid(True, axis='x')\n    plt.show()\n    \nfor name, models in trained_models.items():\n    visualize_importance(models, list(X_train.columns), name)","metadata":{"_uuid":"bf5a4177-475a-4714-8f3b-110bbdb43f47","_cell_guid":"cb60b592-0658-40df-8cd8-5f3e56c83e4f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make Submission","metadata":{"_uuid":"8c4002c9-c10a-47fe-b860-7dc8a84470ee","_cell_guid":"441b89bf-9578-45e2-874d-c993d172eb76","trusted":true}},{"cell_type":"code","source":"sub = pd.read_csv(os.path.join(filepath, 'sample_submission.csv'))\nsub[f'{target_col}'] = test_predss\nsub.to_csv('submission.csv', index=False)\nsub","metadata":{"_uuid":"1b58949a-6b71-4fda-ad23-bee65dd5e0f3","_cell_guid":"06c446e9-8dfc-4096-8d7f-f7b2b2f73837","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(sub[f'{target_col}'])","metadata":{"_uuid":"ed17f991-390a-4348-a6b0-38fed6e023a9","_cell_guid":"5d7d03c9-d3b3-43e0-aa8f-b9561dfdf01a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}